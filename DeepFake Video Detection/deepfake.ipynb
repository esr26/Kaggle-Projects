{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rajku\\anaconda3\\envs\\new_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from facenet_pytorch import MTCNN\n",
    "from PIL import Image\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_path(fake_folder, original_folder):\n",
    "    fake_videos = [fake_folder+'/'+ f for f in os.listdir(fake_folder) if f.endswith(('.mp4', '.avi'))]\n",
    "    original_videos = [original_folder+'/'+ f for f in os.listdir(original_folder) if f.endswith(('.mp4', '.avi'))]\n",
    "    \n",
    "    video_paths = fake_videos + original_videos\n",
    "    labels = [1] * len(fake_videos) + [0] * len(original_videos)\n",
    "\n",
    "    return video_paths, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_folder = \"archive/DFD_manipulated_sequences\"\n",
    "original_folder = \"archive/DFD_original sequences\"\n",
    "video_paths, labels = file_path(fake_folder, original_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2010it [59:55,  1.79s/it]\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.11.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\core\\src\\alloc.cpp:73: error: (-4:Insufficient memory) Failed to allocate 6220800 bytes in function 'cv::OutOfMemoryError'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 92\u001b[0m\n\u001b[0;32m     90\u001b[0m video_paths, labels \u001b[38;5;241m=\u001b[39m get_video_paths_and_labels(fake_folder, original_folder)\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m video_path, label \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mzip\u001b[39m(video_paths, labels)):\n\u001b[1;32m---> 92\u001b[0m     labeled_frames \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_paths\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m     all_data\u001b[38;5;241m.\u001b[39mappend(labeled_frames)\n",
      "Cell \u001b[1;32mIn[4], line 80\u001b[0m, in \u001b[0;36mprocess_video\u001b[1;34m(video_path, label)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mprocess_video\u001b[39m(video_path, label):\n\u001b[1;32m---> 80\u001b[0m     frames \u001b[38;5;241m=\u001b[39m \u001b[43mextract_frames\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m     cropped_faces \u001b[38;5;241m=\u001b[39m crop_faces(frames)\n\u001b[0;32m     84\u001b[0m     processed_faces \u001b[38;5;241m=\u001b[39m normalize_and_resize(cropped_faces)\n",
      "Cell \u001b[1;32mIn[4], line 27\u001b[0m, in \u001b[0;36mextract_frames\u001b[1;34m(video_path, sample_rate, max_size)\u001b[0m\n\u001b[0;32m     25\u001b[0m count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m vid\u001b[38;5;241m.\u001b[39misOpened():\n\u001b[1;32m---> 27\u001b[0m     ret, frame \u001b[38;5;241m=\u001b[39m \u001b[43mvid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ret:\n\u001b[0;32m     29\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.11.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\core\\src\\alloc.cpp:73: error: (-4:Insufficient memory) Failed to allocate 6220800 bytes in function 'cv::OutOfMemoryError'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "from facenet_pytorch import MTCNN\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define folder paths\n",
    "fake_folder = \"archive/DFD_manipulated_sequences\"\n",
    "original_folder = \"archive/DFD_original sequences\"\n",
    "\n",
    "# Function to gather video paths and labels\n",
    "def get_video_paths_and_labels(fake_folder, original_folder):\n",
    "    fake_videos = [os.path.join(fake_folder, f) for f in os.listdir(fake_folder) if f.endswith('.mp4')]\n",
    "    original_videos = [os.path.join(original_folder, f) for f in os.listdir(original_folder) if f.endswith('.mp4')]\n",
    "    video_paths = fake_videos + original_videos\n",
    "    labels = [1] * len(fake_videos) + [0] * len(original_videos)\n",
    "    return video_paths, labels\n",
    "\n",
    "# Extract frames with downscaling\n",
    "def extract_frames(video_path, sample_rate=10, max_size=640):\n",
    "    vid = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    count = 0\n",
    "    while vid.isOpened():\n",
    "        ret, frame = vid.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        if count % sample_rate == 0:\n",
    "            height, width = frame.shape[:2]\n",
    "            if max(height, width) > max_size:\n",
    "                scale = max_size / max(height, width)\n",
    "                frame = cv2.resize(frame, (int(width * scale), int(height * scale)), interpolation=cv2.INTER_AREA)\n",
    "            frames.append(frame)\n",
    "        count += 1\n",
    "    vid.release()\n",
    "    return frames\n",
    "\n",
    "# Batch process with MTCNN on GPU\n",
    "mtcnn = MTCNN(keep_all=True, image_size=224, margin=20, device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def crop_faces(frames):\n",
    "    cropped_faces = []\n",
    "    batch_size = 16  # Process 16 frames at a time\n",
    "    for i in range(0, len(frames), batch_size):\n",
    "        batch = frames[i:i + batch_size]\n",
    "        batch_rgb = [cv2.cvtColor(f, cv2.COLOR_BGR2RGB) for f in batch if f is not None and f.size > 0]\n",
    "        if not batch_rgb:\n",
    "            continue\n",
    "        batch_pil = [Image.fromarray(f) for f in batch_rgb]\n",
    "        try:\n",
    "            boxes, _, _ = mtcnn.detect(batch_pil, landmarks=True)\n",
    "            for j, (frame_rgb, box_set) in enumerate(zip(batch_rgb, boxes)):\n",
    "                if box_set is not None:\n",
    "                    \n",
    "                    for box in box_set:\n",
    "                        x1, y1, x2, y2 = map(int, box)\n",
    "                        x1, y1 = max(0, x1), max(0, y1)\n",
    "                        x2, y2 = min(frame_rgb.shape[1], x2), min(frame_rgb.shape[0], y2)\n",
    "                        if x2 > x1 and y2 > y1:\n",
    "                            face = frame_rgb[y1:y2, x1:x2]\n",
    "                            cropped_faces.append(face)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch {i//batch_size}: {e}\")\n",
    "    return cropped_faces\n",
    "\n",
    "# Normalize and resize faces\n",
    "def normalize_and_resize(faces, size=(224, 224)):\n",
    "    processed_faces = []\n",
    "    for face in faces:\n",
    "        face_img = Image.fromarray(face)\n",
    "        face_img = face_img.resize(size, Image.Resampling.LANCZOS)\n",
    "        face_array = np.array(face_img) / 255.0\n",
    "        processed_faces.append(face_array)\n",
    "    return processed_faces\n",
    "\n",
    "# Process a single video\n",
    "def process_video(video_path, label):\n",
    "    frames = extract_frames(video_path)\n",
    "\n",
    "    cropped_faces = crop_faces(frames)\n",
    "    \n",
    "    processed_faces = normalize_and_resize(cropped_faces)\n",
    "    labeled_data = [(face, label) for face in processed_faces]\n",
    "    return labeled_data\n",
    "\n",
    "# Test with one video\n",
    "all_data = []\n",
    "video_paths, labels = get_video_paths_and_labels(fake_folder, original_folder)\n",
    "for video_path, label in tqdm(zip(video_paths, labels)):\n",
    "    labeled_frames = process_video(video_paths[0], labels[0])\n",
    "    all_data.append(labeled_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logical Cores: 16\n",
      "Physical Cores: 16\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import multiprocessing\n",
    "\n",
    "print(\"Logical Cores:\", os.cpu_count())  # Includes hyper-threading\n",
    "print(\"Physical Cores:\", multiprocessing.cpu_count())  # Excludes hyper-threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
